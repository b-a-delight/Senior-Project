{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "path = 'C:/Users/delightb/Desktop/Final_Project/Senior-Project/Pickle'\n",
    "####   = pd.read_pickle(os.path.join(path,'abvHouse.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'W BOISE' : \n",
    "'NORTH CENTRAL BENCH', 'WEST BOISE', 'WEST BOISE FREEWAY CORRIDOR'\n",
    "\n",
    "#### 'BOISE BENCH' : \n",
    "'CENTRAL BOISE BENCH', 'NORTH CENTRAL BENCH',  'BOISE BENCH FREEWAY CORRIDOR', 'PARK & UNIVERSITY WATERFRONT'\n",
    "\n",
    "#### 'W BOISE-MERIDIAN' : \n",
    "'WEST BOISE', 'NORTHEAST MERIDIAN', 'WEST BOISE FREEWAY CORRIDOR', 'NORTH CENTRAL BENCH','GARDEN CITY CHINDEN CORRIDOR', 'MERIDIAN FREEWAY CORRIDOR'\n",
    "\n",
    "#### 'NORTH BOISE' : \n",
    "'OLD NORTH BOISE', 'BOISE FOOTHILLS', 'NEW NORTH BOISE','DOWNTOWN BOISE', 'WARM SPRINGS','PARK & UNIVERSITY WATERFRONT'\n",
    "\n",
    "#### 'SE BOISE' : \n",
    "'NEW SOUTHEAST BOISE', 'SOUTHEAST BOISE INFILL','EAST BOISE WATERFRONT', 'OREGON TRAIL', 'PARK & UNIVERSITY WATERFRONT', 'SOUTH BOISE DESERT'\n",
    "\n",
    "#### 'NW BOISE-GARDEN CITY' : \n",
    "'NEW NORTH BOISE', 'NORTHWEST BOISE', 'GARDEN CITY WATERFRONT', 'BOISE FOOTHILLS','GARDEN CITY CHINDEN CORRIDOR']\n",
    "\n",
    "#### 'NE BOISE' : \n",
    "'BOISE FOOTHILLS', 'WARM SPRINGS', 'EAST BOISE WATERFRONT', 'BARBER & HARRIS RANCH'\n",
    "\n",
    "#### 'SW BOISE' : \n",
    "'WEST BOISE FREEWAY CORRIDOR', 'AIRPORT', 'BOISE BENCH FREEWAY CORRIDOR', 'SOUTH BOISE DESERT','SOUTHWEST BOISE'\n",
    "\n",
    "#### 'SW BOISE-MERIDIAN': \n",
    "'WEST BOISE FREEWAY CORRIDOR', 'MERIDIAN FREEWAY CORRIDOR', 'SOUTH MERIDIAN', 'SOUTHWEST BOISE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build percent change dataframe\n",
    "Removed all 2000-01-01 because there is no change to observe\n",
    "Removed TAV_Ch <2 because that data was the change of the TAV from a land parcel to a land parcel with a house, which is not the intended measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parcel                                                        0\n",
      "Year                                                          0\n",
      "Group                                                         0\n",
      "Neighborhood                                                  0\n",
      "Area                                                          0\n",
      "Age                                                           0\n",
      "Bedroom_Count                                                 0\n",
      "Bathroom_Count                                                0\n",
      "TAV                                                           0\n",
      "TAV_Ch                                                        0\n",
      "Architecture And Engineering Occupations                      0\n",
      "Arts, Design, Entertainment, Sports, And Media Occupations    0\n",
      "Building And Grounds Cleaning And Maintenance Occupations     0\n",
      "Business And Financial Operations Occupations                 0\n",
      "Community And Social Service Occupations                      0\n",
      "Computer And Mathematical Occupations                         0\n",
      "Construction And Extraction Occupations                       0\n",
      "Education, Training, And Library Occupations                  0\n",
      "Farming, Fishing, And Forestry Occupations                    0\n",
      "Food Preparation And Serving Related Occupations              0\n",
      "Healthcare Practitioner And Technical Occupations             0\n",
      "Healthcare Support Occupations                                0\n",
      "Installation, Maintenance, And Repair Occupations             0\n",
      "Legal Occupations                                             0\n",
      "Life, Physical, And Social Science Occupations                0\n",
      "Management Occupations                                        0\n",
      "Office And Administrative Support Occupations                 0\n",
      "Personal Care And Service Occupations                         0\n",
      "Production Occupations                                        0\n",
      "Protective Service Occupations                                0\n",
      "Sales And Related Occupations                                 0\n",
      "Transportation And Material Moving Occupations                0\n",
      "(10000.0, 40000.0]                                            0\n",
      "(40000.0, 70000.0]                                            0\n",
      "(70000.0, 100000.0]                                           0\n",
      "(100000.0, 150000.0]                                          0\n",
      "(150000.0, 200000.0]                                          0\n",
      "(200000.0, 300000.0]                                          0\n",
      "Total_House_Change                                            0\n",
      "Total_Pop_Change                                              0\n",
      "Total_Emp_Change                                              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "house_df = pd.read_pickle(os.path.join(path,'abvHouse.pkl'))\n",
    "jobs_df = pd.read_pickle(os.path.join(path,'jobShift.pkl'))\n",
    "yearTotals = pd.read_pickle(os.path.join(path,'yrTotalChange.pkl'))\n",
    "\n",
    "df = house_df\n",
    "\n",
    "df = pd.merge(df, jobs_df, on = 'Year', how = 'left')\n",
    "df = pd.merge(df, yearTotals, on = 'Year', how = 'left')\n",
    "\n",
    "df = df[df['Year'] != '2000-01-01']\n",
    "df = df[df['Age']> 1]\n",
    "df = df[pd.notnull(df['Area'])]\n",
    "df = df[df['TAV_Ch']<2]\n",
    "\n",
    "\n",
    "df = df.drop(columns = ['All Occupations'])\n",
    "\n",
    "#df[df.replace([np.inf, -np.inf], np.nan)]\n",
    "\n",
    "\n",
    "#https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/\n",
    "\n",
    "#df.to_pickle('temp.pkl')\n",
    "#df.to_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "print(df.isnull().sum())\n",
    "#df[df.isnull().any(axis=1)]\n",
    "#df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "#df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "109655/1118266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "#df['Architecture And Engineering Occupations'].describe()\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim the TAV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(df):\n",
    "    dfQ1 = df['TAV_Ch'].quantile(0.25)\n",
    "    dfQ3 = df['TAV_Ch'].quantile(0.75)\n",
    "    df_iqr = dfQ3 - dfQ1\n",
    "    iqrMax =dfQ3 + (1.5*df_iqr)\n",
    "    iqrMin = dfQ1 - (1.5*df_iqr)\n",
    "    df_trim = df[(df['TAV_Ch']<iqrMax) & (df['TAV_Ch'] > iqrMin)]\n",
    "    \n",
    "    return df_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "\n",
    "df = trim(df)\n",
    "\n",
    "df.to_pickle(os.path.join(path, 'pct_all_trimmed.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking to verify normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for asymmetry and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010). George, D., & Mallery, M. (2010). SPSS for Windows Step by Step: A Simple Guide and Reference, 17.0 update (10a ed.) Boston: Pearson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before trim, doesn't pass kurtosis after trim it does pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_pickle(os.path.join(path,'pct_all_trimmed.pkl'))\n",
    "sns.distplot(df['TAV_Ch'], fit = norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(df['TAV_Ch'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('TAV distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['TAV_Ch'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(\"Skewness: %f\" % df['TAV_Ch'].skew())\n",
    "print(\"Kurtosis: %f\" % df['TAV_Ch'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "sns.distplot(df['TAV_Ch'], fit = norm);\n",
    "\n",
    "(mu, sigma) = norm.fit(df['TAV_Ch'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('TAV distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['TAV_Ch'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(\"Skewness: %f\" % df['TAV_Ch'].skew())\n",
    "print(\"Kurtosis: %f\" % df['TAV_Ch'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_list = ['NW BOISE-GARDEN CITY']\n",
    "# df = pd.read_pickle('temp.pkl')\n",
    "# df = df[df['Area'] == area_list[0]]\n",
    "\n",
    "# df = trim(df)\n",
    "# #df.head()\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# plt =sns.distplot(df['TAV'])\n",
    "# plt\n",
    "\n",
    "# #df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the correlation of the features to TAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area_list = ['NW BOISE-GARDEN CITY','BOISE BENCH','SE BOISE','NORTH BOISE',\n",
    "#              'NE BOISE', 'W BOISE','SW BOISE-MERIDIAN','W BOISE-MERIDIAN','SW BOISE']\n",
    "area_list = ['W BOISE']\n",
    "\n",
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "df= df.drop(columns = ['Parcel', 'Year','Group', 'Neighborhood','Age','Bedroom_Count','Bathroom_Count'])\n",
    "df.to_pickle('temp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('temp.pkl')\n",
    "coefComp('W BOISE',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path, 'pct_all_trimmed.pkl'))\n",
    "df = df.drop(columns= ['Parcel', 'Group','Neighborhood','TAV'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#found this to clean up the nan and inf problems\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path, 'pct_all_trimmed.pkl'))\n",
    "df = df.drop(columns= ['Parcel', 'Group','Neighborhood'])\n",
    "df = df[df['Area'] != 0]\n",
    "df = clean_dataset(df)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['Area'], prefix='Area')],axis=1)\n",
    "df.drop(['Area'],axis=1, inplace=True)\n",
    "\n",
    "df.to_pickle(os.path.join(path,'oneHotEncode.pkl'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# df = pd.read_pickle(os.path.join(path,'oneHotEncode.pkl'))\n",
    "# df = df[df['Year'] != '2018-01-01']\n",
    "# df = df.drop(columns = ['Year'])\n",
    "# df = clean_dataset(df)\n",
    "\n",
    "# lm_df = df.copy()\n",
    "\n",
    "# feature_cols = df.columns.get_values()\n",
    "# feature_cols = feature_cols.tolist()\n",
    "# feature_cols.remove('TAV_Ch')\n",
    "\n",
    "\n",
    "# y = lm_df.TAV_Ch\n",
    "# X = lm_df[feature_cols]\n",
    "\n",
    "# lm = LinearRegression()\n",
    "# lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep working from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Area Correlation Coefficient\n",
    "use k= 11 because TAV_Ch to TAV_Ch = 1\n",
    "this is trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefComp(area, df):\n",
    "    k = 11\n",
    "    df_Fun = df.copy()\n",
    "    df_Fun = df_Fun[df_Fun['Area'] == area]\n",
    "    df_Fun = trim(df_Fun)\n",
    "    corrmat = df_Fun.corr()\n",
    "    cols = corrmat.nlargest(k, 'TAV_Ch')['TAV_Ch']\n",
    "    temp = pd.DataFrame(cols)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(path,'AN_Build.pkl'))\n",
    "df= df.drop(columns = ['Parcel', 'Year','Group', 'Neighborhood','Age','Bedroom_Count','Bathroom_Count'])\n",
    "temp = list(df.columns)\n",
    "\n",
    "\n",
    "temp = list(df.columns)\n",
    "aBoise = pd.DataFrame(temp)\n",
    "aBoise = aBoise.rename(columns = {0 : 'index'})\n",
    "\n",
    "\n",
    "temp = list(df.columns)\n",
    "aBoise = pd.DataFrame(temp)\n",
    "aBoise = aBoise.rename(columns = {0 : 'index'})\n",
    "\n",
    "df1 = coefComp('NW BOISE-GARDEN CITY', df)\n",
    "df1 = df1.reset_index()\n",
    "aBoise = pd.merge(aBoise,df1, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'NW_Boise_GC'})\n",
    "\n",
    "df2 = coefComp('BOISE BENCH', df)\n",
    "df2 = df2.reset_index()\n",
    "aBoise = pd.merge(aBoise,df2, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'B_Bench'})\n",
    "\n",
    "df3 = coefComp('SE BOISE', df)\n",
    "df3 = df3.reset_index()\n",
    "aBoise = pd.merge(aBoise,df3, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'SE_Boise'})\n",
    "\n",
    "df4 = coefComp('NORTH BOISE', df)\n",
    "df4 = df4.reset_index()\n",
    "aBoise = pd.merge(aBoise,df4, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'N_Boise'})\n",
    "\n",
    "df5 = coefComp('NE BOISE', df)\n",
    "df5 = df5.reset_index()\n",
    "aBoise = pd.merge(aBoise,df5, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'NE_Boise'})\n",
    "\n",
    "df6 = coefComp('W BOISE', df)\n",
    "df6 = df6.reset_index()\n",
    "aBoise = pd.merge(aBoise,df6, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'W_Boise'})\n",
    "\n",
    "df7 = coefComp('SW BOISE-MERIDIAN', df)\n",
    "df7 = df7.reset_index()\n",
    "aBoise = pd.merge(aBoise,df7, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'SW_Boise_M'})\n",
    "\n",
    "df8 = coefComp('W BOISE-MERIDIAN', df)\n",
    "df8 = df8.reset_index()\n",
    "aBoise = pd.merge(aBoise,df8, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'W_Boise_M'})\n",
    "\n",
    "df9 = coefComp('SW BOISE', df)\n",
    "df9 = df9.reset_index()\n",
    "aBoise = pd.merge(aBoise,df9, on = 'index', how = 'left')\n",
    "aBoise = aBoise.rename(columns = {'TAV_Ch' : 'SW_Boise'})\n",
    "\n",
    "\n",
    "aBoise.to_pickle(os.path.join(path,'Trim-A-Coeff-compare.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
